# ðŸŽ™ï¸ Audio Deepfake Detection for Real Conversations

> **Done By**: Dinakar S  
> **Email**: dinakars2003@gmail.com

---

## ðŸ§© Part 1: Research & Selection

### ðŸ“š Reference Repository
I explored the GitHub repository: [Audio Deepfake Detection â€“ media-sec-lab](https://github.com/media-sec-lab/Audio-Deepfake-Detection), which curates a comprehensive collection of papers, models, and datasets focused on audio deepfake detection.

---

### ðŸŽ¯ Use Case
Identify models suitable for:
- âœ… Detecting AI-generated human speech
- âœ… Real-time or near real-time detection
- âœ… Analysis of real-world conversations

---

### âœ… Selected Approaches

---

### **1. AASIST â€“ Audio Anti-Spoofing using Spectro-Temporal Graph Attention**

**What is it?**  
AASIST is a deep learning model using spectro-temporal graphs and attention to catch audio deepfakes. It identifies abnormal pitch or unnatural patterns in both time and frequency dimensions.

**ðŸ” Key Innovations:**
- Graph Attention + Spectro-temporal encoding
- Focuses on suspicious regions using attention
- Generalizes to unseen attacks

**ðŸ“ˆ Performance:**
- EER: **0.83%**, t-DCF: **0.028**

**âœ… Why It Fits:**
- Top-tier accuracy
- Fast inference
- Excellent with background noise and unseen attacks

---

### **2. RawNet2 â€“ End-to-End Model Using Raw Audio**

**What is it?**  
RawNet2 takes raw waveforms and uses Sinc-based filters and GRUs to learn and detect fakes without preprocessing.

**ðŸ” Key Innovations:**
- Raw waveform input (no spectrogram needed)
- Sinc filters + Residual + GRU

**ðŸ“ˆ Performance:**
- EER: **1.12%**, t-DCF: **0.033**

**âœ… Why It Fits:**
- Easy to run with CPU
- No preprocessing required
- Ideal for fast pipelines

**âš ï¸ Limitations:**
- Slower training on CPU
- Lower context understanding

---

### **3. ResMax â€“ Residual Network with Max Feature Map**

**What is it?**  
ResMax uses residual blocks and a Max Feature Map (MFM) layer to detect the most important patterns in audio. It's ultra lightweight and fast.

**ðŸ” Key Innovations:**
- Residual CNN + Max Feature Map activation
- Learns compact, discriminative audio features

**ðŸ“ˆ Performance:**
- LA EER: **2.19%**, PA EER: **0.37% (Rank 1)**

**âœ… Why It Fits:**
- Extremely fast
- Great for mobile & edge devices
- Ideal for replay attack detection

---

### ðŸ“Š Comparison Table

| Feature / Criteria                 | **RawNet2**              | **AASIST**                        | **ResMax**                       |
|-----------------------------------|--------------------------|-----------------------------------|----------------------------------|
| Model Type                        | End-to-End CNN + GRU     | Graph Attention + Hybrid Features | Residual Net + MFM               |
| Input                             | Raw waveform             | Spectrogram                       | CQT (spectrogram)                |
| Performance (EER - LA)            | 1.12%                    | **0.83%**                         | 2.19%                            |
| Real-Time Capability              | Medium                   | High                              | **High**                         |
| Generalization                    | Good                     | **Excellent**                     | Moderate                         |
| Edge Deployment Suitability       | Moderate                 | Good                              | **Best**                         |
| Robustness to Noise               | Medium                   | **High**                          | Low to Moderate                  |

---

## ðŸ› ï¸ Part 2: Implementation

### âœ… Selected Approach: **RawNet2**

### ðŸ“¦ Dataset
- **Dataset**: ASVspoof 2019 Logical Access (LA)
- ðŸ“¥ [Download Link](https://datashare.ed.ac.uk/handle/10283/3336)
- **Used**: LA Train + Dev

### ðŸ’» Environment
- OS: Windows
- RAM: 8 GB
- GPU: âŒ (CPU only)
- Python: 3.6+
- Framework: PyTorch

### ðŸ”§ Modifications
- Used updated PyTorch version (torch 1.10+)
- Converted path formats (Windows)
- Reduced batch size (8) and epochs (3) for CPU

---

### ðŸš€ Training Results

```bash
Epoch 0 | Loss: 0.1503 | Train Acc: 92.50% | Val Acc: 10.26%
Epoch 1 | Loss: 0.0001 | Train Acc: 100.00% | Val Acc: 10.26%
Epoch 2 | Loss: 0.0000 | Train Acc: 100.00% | Val Acc: 10.26%
```

âš ï¸ **Note**: Low validation accuracy is expected due to short training, CPU usage, and no augmentation.

---

### ðŸ“‚ GitHub Repository
ðŸ“Ž [GitHub Code Submission](https://github.com/DINAKAR-S/Audio-Deepfake-Detection-for-Real-Conversations)

---

## ðŸ“š Part 3: Documentation & Analysis

---

### ðŸ”§ Implementation Process

**Challenges:**
- âš ï¸ No GPU â€” slow training
- âš ï¸ PyTorch 1.4 not available
- âš ï¸ Path compatibility issues on Windows

**Solutions:**
- Switched to PyTorch 1.10+
- Adjusted training loop (3 epochs)
- Updated paths and simplified logic

**Assumptions:**
- Short training demonstrates proof of concept
- Validation accuracy is expected to improve with GPU + longer training
- RawNet2 is likely to generalize well with more real-world data

---

### ðŸ”¬ High-Level Explanation: **How RawNet2 Works**

1. **Input**: Raw audio waveform  
2. **Sinc Layer**: Learns frequency filters automatically  
3. **Residual CNN Blocks**: Extract short-term patterns  
4. **GRU Layer**: Captures temporal structure (speech rhythm)  
5. **FC + Softmax**: Outputs spoof/bonafide probability  

ðŸ“Œ All of this is learned directly from audio without hand-crafted features.

---

### ðŸ“ˆ Strengths & Weaknesses

| âœ… Strengths                        | âš ï¸ Weaknesses                          |
|-----------------------------------|----------------------------------------|
| No preprocessing required         | Needs GPU for faster/better training   |
| Lightweight                       | Struggles with unseen noise if untrained |
| Suitable for real-time systems    | Limited context understanding          |

---

### ðŸš€ Future Improvements
- Add noise augmentation (MUSAN, RIR)
- Longer training with GPU
- Add hybrid fusion with AASIST or Wav2Vec2
- Tune thresholds for deployment

---

### ðŸ’¬ Reflection

**1. Most Significant Challenges?**  
Training without GPU and adapting old code for Windows.

**2. Real-World vs Research?**  
Research audio is clean. Real-world has noise, accents, etc. Fine-tuning is required.

**3. What Can Improve It?**  
- More training data  
- Noise augmentation  
- GPU acceleration

**4. Deployment Plan?**
- Convert to ONNX / TorchScript  
- Serve with FastAPI or Flask  
- Run inference on edge or cloud  
- Monitor confidence scores for safety

---

## âœ… Requirements

### ðŸ”§ Setup Instructions
```bash
git clone https://github.com/DINAKAR-S/Audio-Deepfake-Detection-for-Real-Conversations
cd Baseline-RawNet2
pip install -r requirements.txt
python main.py --database_path "./data/LA" --protocols_path "./data/LA"
```

### ðŸ§¾ Dependencies
- torch >= 1.10
- librosa
- numpy
- yaml
- tensorboardX

---

## ðŸ“‚ Dataset Access
- [ASVspoof 2019 Dataset (Logical Access)](https://datashare.ed.ac.uk/handle/10283/3336)
- Place data in this structure:
```
./data/LA/
â”œâ”€â”€ ASVspoof2019_LA_train
â”œâ”€â”€ ASVspoof2019_LA_dev
â”œâ”€â”€ ASVspoof2021_LA_eval
â”œâ”€â”€ ASVspoof_LA_cm_protocols
```

---

---

ðŸ“„ **Attached Full Report**

A full detailed document containing all three parts of the assignment â€” **Research, Implementation, and Analysis** â€” is also attached in this GitHub repository.

ðŸ“Œ **File Name**: `Assignment_Momenta.pdf`

ðŸ‘‰ Kindly go through the PDF for a complete overview of the project, including comparisons, explanations, and reflections.

---
